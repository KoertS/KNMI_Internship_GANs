{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import re\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "from batchcreator import minmax\n",
    "from batchcreator import DataGenerator as dg\n",
    "import tensorflow as tf\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aart Radar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_aart = '/nobackup_1/users/schreurs/project_GAN/dataset_aart'\n",
    "\n",
    "folders = sorted([f for f in os.listdir(path_aart) if os.path.isdir(os.path.join(path_aart, f))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip path:  /nobackup/users/schreurs/project_GAN/dataset_aart/2009/2009.zip\n",
      "unpack in folder:  /nobackup/users/schreurs/project_GAN/dataset_aart/2009\n"
     ]
    }
   ],
   "source": [
    "# Testing path\n",
    "zip_path = path_aart + '/' + folders[0] + '/' + folders[0] + '.zip'\n",
    "print(\"zip path: \", zip_path)\n",
    "unpack_folder = path_aart + '/' + folders[0]\n",
    "print(\"unpack in folder: \", unpack_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip():\n",
    "    for folder in tqdm(folders):\n",
    "        zip_path = path_aart + '/' + folder + '/' + folder + '.zip'\n",
    "        unpack_folder = path_aart + '/' + folder\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "            zip_file.extractall(unpack_folder)\n",
    "#unzip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert NC to npy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays take more memory than nc files. Therefor I will only convert 1 year of data to numpy for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filesnames(start_dt, end_dt):\n",
    "    '''\n",
    "    This function returns filenames between the a starting date and end date. \n",
    "    start_dt: starting date time \n",
    "    end_dt: end date time\n",
    "    '''\n",
    "    # Create list of IDs to retrieve\n",
    "    dts = np.arange(start_dt, end_dt, timedelta(minutes=5)).astype(datetime)\n",
    "    # Convert to filenames\n",
    "    filenames = ['{:%Y%m%d%H%M}'.format(dt) for dt in dts]\n",
    "    return filenames \n",
    "\n",
    "def nc2npy(in_path, out_path, year=2018, as_int=True, overwrite=False, label_dir=None, preprocess=False, filenames=None):\n",
    "    '''\n",
    "    Converts nc files of a given year to numpy files \n",
    "    in_path: path that points to the .nc files\n",
    "    out_path: directory to store numpy files\n",
    "    year: indicates which year to convert\n",
    "    as_int: converts the decimal numbers to integers. \n",
    "            Note that the data is discrete, therefor integers might be more suitable than floats\n",
    "    overwrite: If true then overwrite previously preprocessed data. If false, then skips files that\n",
    "                are already preprocessed\n",
    "    label_dir: directory to store no rain labels. If none then do not label the data\n",
    "    preprocess: If true then preprocesses the data by converting rain to dbz, normalize to [0,1] and rescale to 256x256\n",
    "    '''   \n",
    "    if filenames is not None:\n",
    "        out_path = config.dir_aart_prep\n",
    "    else:\n",
    "        # Get filename of corresponding year\n",
    "        start = datetime(year, 1, 1, 0, 0)\n",
    "        end = datetime(year,12, 31, 23, 55)\n",
    "        filenames = get_filesnames(start,end)\n",
    "        \n",
    "    # Create directory if it does not exist\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    if label_dir and not os.path.exists(label_dir):\n",
    "        os.makedirs(label_dir)\n",
    "    for filename in tqdm(filenames): \n",
    "        year_str = filename[:4]\n",
    "        path_f = in_path + year_str + '/' + config.prefix_aart + filename + '.nc'\n",
    "        \n",
    "        if not overwrite and filename +'.npy' in output_files:\n",
    "            # Skip this file if already processed,\n",
    "            # go to next file in list\n",
    "            continue\n",
    "        try:\n",
    "            with Dataset(path_f, mode='r') as ds:\n",
    "                rain = ds['image1_image_data'][:][0].data\n",
    "                mask = ds['image1_image_data'][:][0].mask\n",
    "                # Apply mask\n",
    "                rain = rain * ~mask\n",
    "        \n",
    "                if as_int:\n",
    "                    rain *=100\n",
    "                    rain = rain.astype(int)\n",
    "                \n",
    "                if preprocess:\n",
    "                    rain = perform_preprocessing(rain)\n",
    "                np.save(out_path + '/{}.npy'.format(filename), rain)    \n",
    "                if label_dir:\n",
    "                    no_rain_thresh =0.3\n",
    "                    if as_int:\n",
    "                        no_rain_thresh = 30\n",
    "                    nr_rainy_pixels = np.sum(rain >= no_rain_thresh)\n",
    "                    no_rain_label = nr_rainy_pixels  == 0\n",
    "                    label_fn = label_dir + '/{}.npy'.format(filename)\n",
    "                    np.save(label_fn, no_rain_label)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            rain = np.zeros((765,700))\n",
    "            if preprocess:\n",
    "                rain = np.zeros((256,256,1))\n",
    "            # np.save(out_path + '/{}.npy'.format(filename), rain)\n",
    "            if label_dir:\n",
    "                no_rain_label = True\n",
    "                label_fn = label_dir + '/{}.npy'.format(filename)\n",
    "                np.save(label_fn, no_rain_label)\n",
    "            print('Error: could not convrt the file {} to numpy'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_preprocessing(y, downscale256=True):\n",
    "    y = minmax(y, tanh=False, undo=False, convert_to_dbz = True)\n",
    "    y = np.expand_dims(y, axis=-1)\n",
    "    if downscale256:\n",
    "        # Temporary expand y dimensions so that cropping function works: h,w,c -> 1,1,h,w,c\n",
    "        y = np.expand_dims(y, axis=0)\n",
    "        y = np.expand_dims(y, axis=0)\n",
    "        # First make the images square size\n",
    "        y = dg.crop_center(dg, y, cropx=384, cropy=384) \n",
    "        \n",
    "        # Remove the extra dimensions \n",
    "        y = y[0][0]\n",
    "        y =  tf.image.resize(y, (256, 256))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4756\n",
      "Approx 0.05 years of data\n"
     ]
    }
   ],
   "source": [
    "path_aart = config.dir_aart\n",
    "path_aart_prep = config.dir_aart_prep\n",
    "\n",
    "# Get files that are already converted to numpy\n",
    "output_files = sorted([f for f in os.listdir(path_aart_prep) \n",
    "                       if os.path.isfile(os.path.join(path_aart_prep, f))])\n",
    "\n",
    "print(len(output_files))\n",
    "print('Approx {:.2f} years of data'.format(len(output_files)/288/365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 3104/4756 [03:06<01:35, 17.31it/s]"
     ]
    }
   ],
   "source": [
    "# Load all target files in the training set\n",
    "fn_aart_train = np.load('train2015_2018.npy', allow_pickle = True)[:,1]\n",
    "fn_aart_val = np.load('val2019.npy', allow_pickle = True)[:,1]\n",
    "\n",
    "filenames_aart = np.append(fn_aart_train, fn_aart_val)\n",
    "# flatten the list\n",
    "filenames_aart = [item for sublist in filenames_aart for item in sublist]\n",
    "\n",
    "nc2npy(path_aart, path_aart_prep, overwrite = True, preprocess=True, filenames=filenames_aart)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
